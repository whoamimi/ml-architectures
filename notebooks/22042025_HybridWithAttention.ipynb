{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f58258",
   "metadata": {},
   "source": [
    "# 22-04-2025: Building Models with Attentions \n",
    "\n",
    "Building hybrid models with Attention block from scratch. Mainly to test my ideas and its usage. I like Attention blocks :)\n",
    "\n",
    "Content\n",
    "- [Gat](#graph-attention-networks-gat-manual-compute)\n",
    "- [Attentions as Decision Model - Experimental](#attentions-as-decision-model)\n",
    "- [Attentions as Decision Model in Grid Search Problems](2204_.ipynb#simulation-grid-example)\n",
    "- [Application with LLM](#application-with-llm)\n",
    "- [Attention Block for Ordinal Predictions](#attention-as-a-ranker)\n",
    "- [Draft Setup for comparing models](#comparing-models-example-setup) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55371a",
   "metadata": {},
   "source": [
    "# Graph Attention Networks (GAT) Manual compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting test x with shape torch.Size([1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdjacencyMatrix:\n",
    "    \"\"\"Undirected Adjacency Matrix representing the edges.\"\"\"\n",
    "\n",
    "    def __init__(self, edge_list):\n",
    "        assert edge_list.dim() == 2 and edge_list.size(0) == edge_list.size(1)\n",
    "\n",
    "        self.A = AdjacencyMatrix.adjacency_to_edge_index(edge_list)\n",
    "        self.A_init = edge_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.A.size(0)\n",
    "\n",
    "    def __getitem__(self, node_index: int, node_neighbor: int = None):\n",
    "        assert -len(self) < node_index < len(self), (\n",
    "            f\"Inserted {node_index} out of scope.\"\n",
    "        )\n",
    "\n",
    "        if node_neighbor:\n",
    "            assert -len(self) < node_neighbor < len(self), (\n",
    "                f\"Inserted {node_neighbor} out of scope.\"\n",
    "            )\n",
    "            return self.A[node_index][node_neighbor]\n",
    "\n",
    "        return self.A[node_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def adjacency_to_edge_index(adj: torch.Tensor) -> torch.Tensor:\n",
    "        # Get indices where there are edges (nonzero entries)\n",
    "        src, tgt = torch.nonzero(adj, as_tuple=True)\n",
    "        edge_index = torch.stack([src, tgt], dim=0)\n",
    "        return edge_index\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"Input dim = Feature vector dimension that exists at each node.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_node: int, input_dim: int, output_dim: int, edge_list: torch.Tensor\n",
    "    ):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_node = num_node  # number of neighbourhoods\n",
    "        self.feature_dim = input_dim  # feature_dim\n",
    "        self.output_dim = (\n",
    "            output_dim  # output_dim e.g. num of labels for classification tasks\n",
    "        )\n",
    "        self.edges = AdjacencyMatrix(edge_list)\n",
    "\n",
    "        # alternately for the following 2 lines: nn.Linear(self.feature_dim, self.output_dim, bias=False)\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(self.output_dim, self.feature_dim, requires_grad=False)\n",
    "        )\n",
    "        # self.bias = nn.Parameter(torch.ones(self.m))\n",
    "\n",
    "        self.fn = nn.Linear(self.num_node, self.feature_dim, bias=False)\n",
    "        self.attn = nn.Parameter(\n",
    "            torch.empty(size=(2 * self.output_dim, 1)), requires_grad=False\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.attn, gain=1.414)\n",
    "        self.active = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert inputs.dim() == 3, (\n",
    "            f\"Input vector must be of shape: [batch_size, {self.num_node}, {self.feature_dim}]\"\n",
    "        )\n",
    "        assert inputs.size(1) == self.num_node and inputs.size(2) == self.feature_dim, (\n",
    "            f\"Expected input feature `({self.num_node}, {self.feature_dim})` (num of nodes, feature dim) but received {inputs.shape}\"\n",
    "        )\n",
    "\n",
    "        num_batch = inputs.size(0)\n",
    "        e_i, e_j = self.edges.A\n",
    "        e_i = (\n",
    "            e_i.unsqueeze(0).repeat(batch_size, 1)\n",
    "            + (torch.arange(batch_size) * self.num_node).unsqueeze(1).flatten()\n",
    "        )\n",
    "        e_j = (\n",
    "            e_j.unsqueeze(0).repeat(batch_size, 1)\n",
    "            + (torch.arange(batch_size) * self.num_node).unsqueeze(1).flatten()\n",
    "        )\n",
    "\n",
    "        x = inputs @ self.W.T  # [batch_size, num_node, self.output_dim]\n",
    "        assert (\n",
    "            x.size(0) == num_batch\n",
    "            and x.size(1) == self.num_node\n",
    "            and x.size(2) == self.output_dim\n",
    "        ), f\"Incorrect output after linear with shape {x.shape}. Returned: {x}\"\n",
    "        x = x.view(-1, self.output_dim)\n",
    "\n",
    "        # create attn inputs for edges\n",
    "        a_input = torch.cat(\n",
    "            [x[e_i], x[e_j]], dim=1\n",
    "        )  # [2 * self.output_dim, 2 * self.output_dim]\n",
    "        assert a_input.size(0) == a_input.size(1) == 2 * self.output_dim, (\n",
    "            f\"Error concatenating: unexpected size of ({a_input.shape}). Output\\n{a_input} \"\n",
    "        )\n",
    "\n",
    "        a = self.active(torch.matmul(a_input, self.attn)).squeeze(-1)  # [2 * num_node]\n",
    "        assert a.dim() == 1 and a.size(0) == 2 * self.output_dim, (\n",
    "            f\"Expected size {2 * self.output_dim} but received: {a.shape}\"\n",
    "        )\n",
    "        print(f\"Alpha {a}\")\n",
    "\n",
    "        # Softmax over neighbor edges\n",
    "        denom = torch.zeros(num_batch * self.num_node, device=x.device).scatter_add(\n",
    "            0, e_i, a\n",
    "        )\n",
    "        print(f\"Softmaxed before dropout (shape {denom.shape}): {denom}\")\n",
    "\n",
    "        alpha = self.dropout(\n",
    "            a / (denom[e_i] + 1e-16)\n",
    "        )  # actually used self.dropout(torch.ones_like(a) / (denom[e_i] + 1e-16))\n",
    "        print(f\"Coeff (shape {alpha.shape}): {alpha}\")\n",
    "\n",
    "        # Aggregate features\n",
    "        output = torch.zeros_like(x)\n",
    "        output.index_add_(0, e_i, alpha.unsqueeze(-1) * x[e_j])\n",
    "\n",
    "        return output, a\n",
    "\n",
    "\n",
    "adj = torch.tensor(\n",
    "    [[0, 1, 1, 0], [1, 0, 0, 2], [5, 0, 0, 1], [0, 1, 1, 0]], dtype=torch.float32\n",
    ")\n",
    "\n",
    "num_node = 4\n",
    "input_dim = 1\n",
    "output_dim = 4\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [1.0],  # Node 0\n",
    "            [10.0],  # Node 1\n",
    "            [3.0],  # Node 2\n",
    "            [4.0],\n",
    "        ]  # Node 3\n",
    "    ]\n",
    ")  # Shape: (1, 4, 1)\n",
    "\n",
    "print(f\"Inserting test x with shape {x.shape}\")\n",
    "model = GAT(\n",
    "    num_node=num_node, input_dim=input_dim, output_dim=output_dim, edge_list=adj\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef9b6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha tensor([-0.1224, -0.0311,  6.6841,  2.7713,  1.0922, -0.0282, -0.0985, -0.0072],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Softmaxed before dropout (shape torch.Size([4])): tensor([-0.1536,  9.4554,  1.0640, -0.1056], grad_fn=<ScatterAddBackward0>)\n",
      "Coeff (shape torch.Size([8])): tensor([0.0000, 0.0000, 1.7673, 0.7327, 2.5663, -0.0000, 0.0000, 0.1698],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.0272, 0.0277, 0.0740, 0.8711],\n",
       "        [0.0963, 0.0973, 0.1664, 0.6399],\n",
       "        [0.2187, 0.2191, 0.2437, 0.3184]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, a = model(x)\n",
    "output.softmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b57ce2",
   "metadata": {},
   "source": [
    "# Attentions as Decision Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61823b2f",
   "metadata": {},
   "source": [
    "Manual implementation of Attention (number of heads = 1). Alternatively, can use `nn.MultiheadAttention` with 1 head specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936619f2",
   "metadata": {},
   "source": [
    "#### Self Attention Manually computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcaf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV shapes: torch.Size([1, 3, 8]) torch.Size([1, 3, 8]) torch.Size([1, 3, 8])\n",
      "Attn scores torch.Size([1, 3, 3]) Attn weights torch.Size([1, 3, 3]) Attn output pre torch.Size([1, 3, 8]) Attn output torch.Size([1, 3, 1])\n",
      "Output:\n",
      " tensor([[[0.1673],\n",
      "         [0.1680],\n",
      "         [0.1667]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Attention Scores:\n",
      " tensor([[[-0.0257, -0.0357, -0.0475],\n",
      "         [-0.0135, -0.0113, -0.0030],\n",
      "         [-0.0492, -0.0550, -0.0850]]], grad_fn=<DivBackward0>)\n",
      "Attention Weights:\n",
      " tensor([[[0.3369, 0.3335, 0.3296],\n",
      "         [0.3319, 0.3326, 0.3354],\n",
      "         [0.3379, 0.3360, 0.3261]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_block: int, feat_dim: int, output_dim: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_block = num_block\n",
    "        self.feat_dim = feat_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.scale_term = self.feat_dim**0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(self.feat_dim, self.feat_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.feat_dim, self.feat_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.feat_dim, self.feat_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(feat_dim, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs shape = (B, T, D) = (batch_dim, seq_length, self.output_dim)\n",
    "        assert x.dim() == 3 and x.size(-1) == self.feat_dim, (\n",
    "            f\"Expected input shape (batch_size, seq_length, {self.feat_dim}) but received {x.shape}\"\n",
    "        )\n",
    "\n",
    "        # x: (batch_size, seq_len, self.feat_dim)\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        print(\"QKV shapes:\", Q.shape, K.shape, V.shape)\n",
    "\n",
    "        # Attention: Q (B,T,D) @ Káµ€ (B,D,L) => (B,T,L)\n",
    "        attn_scores = Q @ K.transpose(-2, -1) / self.scale_term  # Scaled dot-product\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_output_ = attn_weights @ V\n",
    "        attn_output = self.out_proj(attn_output_)\n",
    "        print(\n",
    "            f\"Attn scores {attn_scores.shape} Attn weights {attn_weights.shape} Attn output pre {attn_output_.shape} Attn output {attn_output.shape}\"\n",
    "        )\n",
    "\n",
    "        return attn_output, attn_scores, attn_weights\n",
    "\n",
    "\n",
    "def trial_run():\n",
    "    \"\"\"To test the model architecture\"\"\"\n",
    "\n",
    "    seq_len = 4\n",
    "    num_block = 30\n",
    "    feat_dim = 8  # does it have to be same as seq length?\n",
    "    output_dim = 1\n",
    "    x = torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                [0.1, 0.2, 0.3, 0.4, 0.0, 0.1, 0.2, 0.3],\n",
    "                [0.3, 0.2, 0.1, 0.0, 0.4, 0.3, 0.2, 0.1],\n",
    "                [0.5, 0.6, 0.7, 0.8, 0.2, 0.1, 0.0, 0.3],\n",
    "            ]\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [1, seq_len, output_dim]\n",
    "\n",
    "    model = SelfAttention(num_block=1, feat_dim=feat_dim, output_dim=output_dim)\n",
    "    output, scores, weights = model(x)\n",
    "\n",
    "    print(\"Output:\\n\", output)\n",
    "    print(\"Attention Scores:\\n\", scores)\n",
    "    print(\"Attention Weights:\\n\", weights)\n",
    "\n",
    "\n",
    "trial_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cache = output.past_key_values.key_cache\n",
    "v_cache = output.past_key_values.value_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "628eb151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3, 54, 64])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat(k_cache).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f4445",
   "metadata": {},
   "source": [
    "#### Simple Simulation Grid Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_maze(maze):\n",
    "    H, W = maze.shape\n",
    "    features = []\n",
    "\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            val = maze[i, j]\n",
    "            features.append(\n",
    "                [\n",
    "                    float(val == 2),  # is_start\n",
    "                    float(val == 3),  # is_goal\n",
    "                    float(val == 1),  # is_wall\n",
    "                    i / H,  # normalized y\n",
    "                    j / W,  # normalized x\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return torch.tensor([features], dtype=torch.float32)  # [1, seq_len, feat_dim]\n",
    "\n",
    "\n",
    "def agent_step(attn_weights, maze, H, W):\n",
    "    # Find index of start\n",
    "    flat_maze = maze.flatten()\n",
    "    start_idx = (flat_maze == 2).nonzero().item()\n",
    "\n",
    "    # Look at attention from start node to all others\n",
    "    attn_from_start = attn_weights[0, start_idx]\n",
    "\n",
    "    # Mask walls\n",
    "    wall_mask = flat_maze == 1\n",
    "    attn_from_start[wall_mask] = -float(\"inf\")\n",
    "\n",
    "    # Choose highest attention score (simulating agent move)\n",
    "    next_idx = attn_from_start.argmax().item()\n",
    "    next_pos = (next_idx // W, next_idx % W)\n",
    "\n",
    "    return next_pos\n",
    "\n",
    "\n",
    "def trial_as_decision():\n",
    "    maze = torch.tensor(\n",
    "        [[2, 0, 1], [1, 0, 3], [0, 0, 1]]\n",
    "    )  # 2=start, 3=goal, 1=wall ~ goal would be [1, 2]\n",
    "\n",
    "    H, W = maze.shape\n",
    "    x = encode_maze(maze)\n",
    "\n",
    "    num_block = 1\n",
    "    feat_dim = 5\n",
    "    model = SelfAttention(num_block=1, feat_dim=feat_dim, output_dim=feat_dim)\n",
    "    output, scores, weights = model(x)\n",
    "\n",
    "    next_pos = agent_step(weights, maze, H, W)\n",
    "    print(\"Agent should move to:\", next_pos)\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Attention Scores:\", scores)\n",
    "    print(\"Attention Weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1368b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV shapes: torch.Size([1, 9, 5]) torch.Size([1, 9, 5]) torch.Size([1, 9, 5])\n",
      "Attn scores torch.Size([1, 9, 9]) Attn weights torch.Size([1, 9, 9]) Attn output pre torch.Size([1, 9, 5]) Attn output torch.Size([1, 9, 5])\n",
      "Agent should move to: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "trial_as_decision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe8815",
   "metadata": {},
   "source": [
    "#### Simulation Grid Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Maze:\n",
      " tensor([[0, 0, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 2],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "QKV shapes: torch.Size([1, 25, 5]) torch.Size([1, 25, 5]) torch.Size([1, 25, 5])\n",
      "Attn scores torch.Size([1, 25, 25]) Attn weights torch.Size([1, 25, 25]) Attn output pre torch.Size([1, 25, 5]) Attn output torch.Size([1, 25, 5])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0], dtype=torch.int32)\n",
      "Epoch 0 | Return: -1.00\n",
      "QKV shapes: torch.Size([1, 25, 5]) torch.Size([1, 25, 5]) torch.Size([1, 25, 5])\n",
      "Attn scores torch.Size([1, 25, 25]) Attn weights torch.Size([1, 25, 25]) Attn output pre torch.Size([1, 25, 5]) Attn output torch.Size([1, 25, 5])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0], dtype=torch.int32)\n",
      "QKV shapes: torch.Size([1, 25, 5]) torch.Size([1, 25, 5]) torch.Size([1, 25, 5])\n",
      "Attn scores torch.Size([1, 25, 25]) Attn weights torch.Size([1, 25, 25]) Attn output pre torch.Size([1, 25, 5]) Attn output torch.Size([1, 25, 5])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 18 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     train(agent, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, maze_size\u001b[38;5;241m=\u001b[39mmaze_size)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[215], line 107\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m agent \u001b[38;5;241m=\u001b[39m MazeAttentionAgent(num_block\u001b[38;5;241m=\u001b[39mnum_block, feat_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mfeat_dim)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaze_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[215], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, epochs, maze_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m flat_maze \u001b[38;5;241m=\u001b[39m maze\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(flat_maze)\n\u001b[0;32m---> 49\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mflat_maze\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_ACTION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m start_pos \u001b[38;5;241m=\u001b[39m (start_idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m maze\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], start_idx \u001b[38;5;241m%\u001b[39m maze\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     52\u001b[0m pos \u001b[38;5;241m=\u001b[39m start_pos\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 18 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "NUM_ACTION = 4\n",
    "\n",
    "\n",
    "class MazeAttentionAgent(nn.Module):\n",
    "    def __init__(self, num_block: int, feat_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(num_block, feat_dim, output_dim)\n",
    "        self.policy_head = nn.Linear(feat_dim, NUM_ACTION)  # 4 = up/down/left/right\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _, attn_weights = self.attn(x)\n",
    "        logits = self.policy_head(attn_output)  # (B, N, 4)\n",
    "        return logits, attn_weights\n",
    "\n",
    "\n",
    "def generate_random_maze(H, W):\n",
    "    maze = torch.zeros(H, W, dtype=torch.int32)\n",
    "    maze[random.randint(0, H - 1), random.randint(0, W - 1)] = 2  # start\n",
    "    maze[random.randint(0, H - 1), random.randint(0, W - 1)] = 3  # goal\n",
    "    for _ in range(int(H * W * 0.3)):\n",
    "        maze[random.randint(0, H - 1), random.randint(0, W - 1)] = 1  # walls\n",
    "\n",
    "    print(f\"Generated Maze:\\n\", maze)\n",
    "    return maze\n",
    "\n",
    "\n",
    "def is_valid(pos, maze):\n",
    "    y, x = pos\n",
    "    return 0 <= y < maze.shape[0] and 0 <= x < maze.shape[1] and maze[y, x] != 1\n",
    "\n",
    "\n",
    "def move(pos, action_idx):\n",
    "    dy, dx = ACTIONS[action_idx]\n",
    "    return (pos[0] + dy, pos[1] + dx)\n",
    "\n",
    "\n",
    "def train(agent, epochs=1000, maze_size=(5, 5)):\n",
    "    gamma = 0.99  # discount factor\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=1e-3)\n",
    "    maze = generate_random_maze(*maze_size)\n",
    "    encoded = encode_maze(maze)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logits, _ = agent(encoded)\n",
    "\n",
    "        flat_maze = maze.flatten()\n",
    "        print(flat_maze)\n",
    "        start_idx = (flat_maze == random.choice(range(NUM_ACTION))).nonzero().item()\n",
    "        start_pos = (start_idx // maze.shape[1], start_idx % maze.shape[1])\n",
    "\n",
    "        pos = start_pos\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        max_steps = 10\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            idx = pos[0] * maze.shape[1] + pos[1]\n",
    "            probs = logits[0, idx].softmax(-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            next_pos = move(pos, action.item())\n",
    "\n",
    "            if not is_valid(next_pos, maze):\n",
    "                rewards.append(-1.0)\n",
    "                log_probs.append(dist.log_prob(action))\n",
    "                break\n",
    "\n",
    "            pos = next_pos\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "\n",
    "            if maze[pos[0], pos[1]] == 3:\n",
    "                rewards.append(10.0)\n",
    "                break\n",
    "            else:\n",
    "                rewards.append(-0.1)\n",
    "\n",
    "        # Discounted rewards\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        # Policy gradient loss\n",
    "        loss = 0\n",
    "        for log_prob, ret in zip(log_probs, returns):\n",
    "            loss -= log_prob * ret\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Return: {sum(rewards):.2f}\")\n",
    "\n",
    "\n",
    "def run():\n",
    "    num_block = 1\n",
    "    feat_dim = 5\n",
    "    num_epochs = 1000\n",
    "    maze_size = (5, 5)\n",
    "    # Initialize the agent\n",
    "    agent = MazeAttentionAgent(num_block=num_block, feat_dim=5, output_dim=feat_dim)\n",
    "    # Train the agent\n",
    "    train(agent, epochs=num_epochs, maze_size=maze_size)\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40693315",
   "metadata": {},
   "source": [
    "# Application with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa656f",
   "metadata": {},
   "source": [
    "Sequence Length in Multi-head Attentions\n",
    "\n",
    "- In most LLM applications, the multi-head attentions accepts input size of (batch_size, seq_length, emb_dim)\n",
    "- The `seq_length` (sequence length) means the number of channels and the `emb_dim` represents the vector representation per sequence input or channel (and NOT the batch input feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446db637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DynamicCache,\n",
    "    GenerationConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f338741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens length: 50\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 119 dims from 26 to 145\n",
      "Hidden states shape: torch.Size([1, 50, 576])\n",
      "Signal from Attn torch.Size([1, 50, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 153 dims from 327 to 480\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 280 dims from 10 to 290\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 240 dims from 158 to 398\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 267 dims from 234 to 501\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 57 dims from 406 to 463\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 70 dims from 101 to 171\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 101 dims from 363 to 464\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 279 dims from 63 to 342\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 162 dims from 275 to 437\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 207 dims from 170 to 377\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 288 dims from 239 to 527\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 115 dims from 30 to 145\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 193 dims from 4 to 197\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 92 dims from 284 to 376\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 285 dims from 61 to 346\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 99 dims from 447 to 546\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 134 dims from 16 to 150\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 123 dims from 312 to 435\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 146 dims from 98 to 244\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 157 dims from 290 to 447\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 282 dims from 184 to 466\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 122 dims from 395 to 517\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 216 dims from 169 to 385\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 227 dims from 303 to 530\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 141 dims from 69 to 210\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 143 dims from 156 to 299\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 59 dims from 429 to 488\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 54 dims from 387 to 441\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 233 dims from 42 to 275\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 254 dims from 183 to 437\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 269 dims from 209 to 478\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 190 dims from 300 to 490\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 256 dims from 1 to 257\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 263 dims from 285 to 548\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 155 dims from 376 to 531\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 252 dims from 163 to 415\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 247 dims from 207 to 454\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 196 dims from 127 to 323\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 117 dims from 369 to 486\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 63 dims from 486 to 549\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 265 dims from 128 to 393\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 184 dims from 328 to 512\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 174 dims from 334 to 508\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 239 dims from 37 to 276\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 238 dims from 67 to 305\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 80 dims from 318 to 398\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 61 dims from 340 to 401\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 251 dims from 220 to 471\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 273 dims from 151 to 424\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output tokens (post squeeze): torch.Size([100])\n",
      "Response: I'm so sorry to hear that you're having a tough time. I'm here to help you find a therapist. I'm here to listen to you, to support you, and to make sure you're getting the help you need.\n",
      "\n",
      "\n",
      "--------------------\n",
      "Full output:\n",
      " <|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_start|>system\n",
      "You are Naomi, a friendly and sarcastic human chatting with users online!<|im_end|>\n",
      "<|im_start|>user\n",
      "Hey Naomi, I fucking hate you..<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm so sorry to hear that you're having a tough time. I'm here to help you find a therapist. I'm here to listen to you, to support you, and to make sure you're getting the help you need.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_config = dict(\n",
    "    use_cache=True,\n",
    "    output_attentions=False,\n",
    "    return_dict_in_generate=True,\n",
    "    return_legacy_cache=False,\n",
    ")\n",
    "decode_config = dict(max_new_tokens=50, do_sample=False, stop_strings=\"\\n\\n\")\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, system_prompt=None):\n",
    "        if system_prompt is None:\n",
    "            prompt = \"You are Naomi, a friendly and sarcastic human chatting with users online!\"\n",
    "        else:\n",
    "            prompt = system_prompt\n",
    "\n",
    "        self.history = [{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "    def chat(self, user_input: str):\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            self.history, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = tokenizer.encode(\n",
    "            input_text,\n",
    "            max_length=50,\n",
    "            padding=\"max_length\",\n",
    "            padding_side=\"left\",\n",
    "            truncation=\"longest_first\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        end_prompt_idx = inputs.size(1)  # type: ignore\n",
    "        print(f\"Input tokens length: {end_prompt_idx}\")\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            tokenizer=tokenizer,\n",
    "            **decode_config,\n",
    "            **output_config,  # type: ignore\n",
    "        )\n",
    "\n",
    "        output_token = outputs.sequences.squeeze(0)\n",
    "        print(f\"Output tokens (post squeeze): {output_token.shape}\")\n",
    "        response = tokenizer.decode(\n",
    "            output_token[end_prompt_idx:].cpu().numpy(), skip_special_tokens=True\n",
    "        )\n",
    "        text = response.replace(\"assistant\\n\", \"\")\n",
    "        print(f\"Response: {text}\")\n",
    "        print(\"--\" * 10)\n",
    "        print(\n",
    "            \"Full output:\\n\",\n",
    "            tokenizer.decode(outputs.sequences.squeeze(0).cpu().numpy()),\n",
    "        )\n",
    "        # odict_keys(['sequences', 'attentions', 'past_key_values']) - it must output attentions to enable use_cache=True (return past_key_values)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def signal_random_injection(mod_signal):\n",
    "        num_slices = torch.randint(low=50, high=300, size=(1,)).item()\n",
    "\n",
    "        # Inject noise into a random slice of the last dimension\n",
    "        # Select a start index randomly such that it doesn't overflow\n",
    "        start_idx = torch.randint(0, mod_signal.size(-1) - num_slices, (1,)).item()\n",
    "        end_idx = start_idx + num_slices\n",
    "\n",
    "        # Create your modification signal (e.g., ones or random noise)\n",
    "        injected_patch = torch.randn_like(mod_signal[:, :, start_idx:end_idx]) * 50\n",
    "\n",
    "        # Inject the patch\n",
    "        mod_signal[:, :, start_idx:end_idx] += injected_patch\n",
    "\n",
    "        print(f\"Injected {num_slices} dims from {start_idx} to {end_idx}\")\n",
    "        return mod_signal\n",
    "\n",
    "    @staticmethod\n",
    "    def injection_layer(module, input, output=None):\n",
    "        print(\n",
    "            f\"{'--' * 10} Injection layer summary {'--' * 10}\\nInput: {len(input) if input else None}\\nOutput: {len(output) if output else None}\"\n",
    "        )\n",
    "        hidden_states = input[0]\n",
    "        mod_signal = torch.zeros_like(\n",
    "            hidden_states\n",
    "        )  # one of the methods is to multiply by some bias * 0.1\n",
    "        mod_signal = ModelManager.signal_random_injection(\n",
    "            mod_signal\n",
    "        )  # mod_signal[:, :, :100] = 100\n",
    "        print(\n",
    "            f\"Hidden states shape: {hidden_states.shape}\\nSignal from Attn {mod_signal.shape}\"\n",
    "        )\n",
    "        print(\"--\" * 50)\n",
    "        return hidden_states + mod_signal\n",
    "\n",
    "    def setup(self, hook_layer: int):\n",
    "        model.model.layers[\n",
    "            hook_layer\n",
    "        ].post_attention_layernorm.register_forward_pre_hook(\n",
    "            ModelManager.injection_layer\n",
    "        )\n",
    "\n",
    "\n",
    "target_layer = -1\n",
    "treadmill = ModelManager()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM-135M-Instruct\", attn_implementation=\"eager\"\n",
    ")\n",
    "model.eval()\n",
    "treadmill.setup(target_layer)\n",
    "output = treadmill.chat(\"Hey Naomi, I fucking hate you..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "b3daa6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens length: 50\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 247 dims from 241 to 488\n",
      "Hidden states shape: torch.Size([1, 50, 576])\n",
      "Signal from Attn torch.Size([1, 50, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 61 dims from 20 to 81\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 197 dims from 300 to 497\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 264 dims from 310 to 574\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 290 dims from 4 to 294\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 298 dims from 152 to 450\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 299 dims from 87 to 386\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 241 dims from 69 to 310\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 193 dims from 377 to 570\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 143 dims from 32 to 175\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------- Injection layer summary --------------------\n",
      "Input: 1\n",
      "Output: None\n",
      "Injected 174 dims from 252 to 426\n",
      "Hidden states shape: torch.Size([1, 1, 576])\n",
      "Signal from Attn torch.Size([1, 1, 576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output tokens (post squeeze): torch.Size([61])\n",
      "Response:  time. I'm here to help you.\n",
      "\n",
      "\n",
      "--------------------\n",
      "Full output:\n",
      " <|im_start|>system\n",
      "You are Naomi, a friendly and sarcastic human chatting with users online!<|im_end|>\n",
      "<|im_start|>user\n",
      "Hey Naomi, I fucking hate you..<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm so sorry to hear that you're having a tough time. I'm here to help you.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateDecoderOnlyOutput(sequences=tensor([[    1,  9690,   198,  2683,   359, 45242,    28,   253,  7952,   284,\n",
       "         24459,  2607,  1205, 36498,   351,  3629,  2329,    17,     2,   198,\n",
       "             1,  4093,   198, 22234, 45242,    28,   339,   275, 24431, 13735,\n",
       "           346,   950,     2,   198,     1,   520,  9531,   198,    57,  5248,\n",
       "           588, 22657,   288,  4875,   338,   346,  2316,  1953,   253,  9228,\n",
       "           655,    30,   339,  5248,  1535,   288,   724,   346,    30,   198,\n",
       "           198]]), scores=None, logits=None, attentions=None, hidden_states=None, past_key_values=DynamicCache())"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treadmill.chat(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525757a",
   "metadata": {},
   "source": [
    "# Attention as a Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aff622",
   "metadata": {},
   "source": [
    "- Full script in macbooks local `algorithms`\n",
    "- The Ranker model was supposed to rank by inputting 1s and 0s but it seems that it could perform better as a multi-label classifier (when the loss function  is nn.BCEWithLogitsLoss) with sigmoid output \n",
    "- The Ranker model works when the loss is MSELoss and the output is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f919e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 25, 50])\n",
      "torch.Size([10, 25, 50])\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 1/100 - Loss: 0.2565\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 2/100 - Loss: 0.2527\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 3/100 - Loss: 0.2495\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 4/100 - Loss: 0.2460\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 5/100 - Loss: 0.2429\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 6/100 - Loss: 0.2400\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 7/100 - Loss: 0.2370\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 8/100 - Loss: 0.2341\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 9/100 - Loss: 0.2318\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 10/100 - Loss: 0.2291\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 11/100 - Loss: 0.2267\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 12/100 - Loss: 0.2242\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 13/100 - Loss: 0.2226\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 14/100 - Loss: 0.2199\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 15/100 - Loss: 0.2182\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 16/100 - Loss: 0.2160\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 17/100 - Loss: 0.2140\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 18/100 - Loss: 0.2119\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 19/100 - Loss: 0.2098\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 20/100 - Loss: 0.2079\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 21/100 - Loss: 0.2059\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 22/100 - Loss: 0.2038\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 23/100 - Loss: 0.2014\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 24/100 - Loss: 0.1999\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 25/100 - Loss: 0.1973\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 26/100 - Loss: 0.1953\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 27/100 - Loss: 0.1929\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 28/100 - Loss: 0.1906\n",
      "Model predicted ranks:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 29/100 - Loss: 0.1880\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 30/100 - Loss: 0.1860\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 31/100 - Loss: 0.1820\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 32/100 - Loss: 0.1809\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 33/100 - Loss: 0.1776\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 34/100 - Loss: 0.1743\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 35/100 - Loss: 0.1719\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 36/100 - Loss: 0.1696\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 37/100 - Loss: 0.1654\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 38/100 - Loss: 0.1624\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 39/100 - Loss: 0.1562\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 2]) Number of correct 2 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 40/100 - Loss: 0.1533\n",
      "Model predicted ranks:  tensor([2, 3, 2, 2, 2, 2, 2, 3, 2, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 41/100 - Loss: 0.1515\n",
      "Model predicted ranks:  tensor([3, 3, 2, 2, 2, 2, 2, 3, 1, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 42/100 - Loss: 0.1466\n",
      "Model predicted ranks:  tensor([3, 3, 2, 2, 2, 2, 2, 3, 1, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 43/100 - Loss: 0.1403\n",
      "Model predicted ranks:  tensor([3, 3, 2, 2, 1, 2, 2, 3, 1, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 44/100 - Loss: 0.1428\n",
      "Model predicted ranks:  tensor([3, 3, 1, 2, 1, 1, 2, 3, 1, 2]) Number of correct 5 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 45/100 - Loss: 0.1331\n",
      "Model predicted ranks:  tensor([3, 3, 1, 1, 1, 1, 1, 3, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 46/100 - Loss: 0.1302\n",
      "Model predicted ranks:  tensor([3, 3, 1, 1, 1, 1, 1, 3, 1, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 47/100 - Loss: 0.1278\n",
      "Model predicted ranks:  tensor([3, 3, 1, 1, 1, 1, 1, 3, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 48/100 - Loss: 0.1241\n",
      "Model predicted ranks:  tensor([3, 3, 1, 1, 1, 1, 1, 3, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 49/100 - Loss: 0.1205\n",
      "Model predicted ranks:  tensor([3, 3, 1, 2, 1, 1, 1, 3, 0, 1]) Number of correct 5 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 50/100 - Loss: 0.1204\n",
      "Model predicted ranks:  tensor([2, 3, 1, 1, 1, 1, 1, 3, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 51/100 - Loss: 0.1119\n",
      "Model predicted ranks:  tensor([2, 2, 1, 1, 1, 1, 1, 3, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 52/100 - Loss: 0.1117\n",
      "Model predicted ranks:  tensor([2, 2, 1, 2, 1, 1, 1, 2, 0, 2]) Number of correct 4 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 53/100 - Loss: 0.1051\n",
      "Model predicted ranks:  tensor([2, 2, 1, 2, 1, 1, 2, 2, 0, 2]) Number of correct 5 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 54/100 - Loss: 0.0989\n",
      "Model predicted ranks:  tensor([2, 2, 1, 1, 1, 1, 1, 2, 0, 2]) Number of correct 3 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 55/100 - Loss: 0.0979\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 1, 2, 0, 2]) Number of correct 5 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 56/100 - Loss: 0.0947\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 57/100 - Loss: 0.0926\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 58/100 - Loss: 0.0903\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 59/100 - Loss: 0.0881\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 60/100 - Loss: 0.0817\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 1, 2, 0, 2]) Number of correct 5 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 61/100 - Loss: 0.0760\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 62/100 - Loss: 0.0746\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 63/100 - Loss: 0.0765\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 64/100 - Loss: 0.0709\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 65/100 - Loss: 0.0699\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 66/100 - Loss: 0.0729\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 67/100 - Loss: 0.0628\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 68/100 - Loss: 0.0602\n",
      "Model predicted ranks:  tensor([3, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 69/100 - Loss: 0.0603\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 70/100 - Loss: 0.0589\n",
      "Model predicted ranks:  tensor([2, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 7 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 71/100 - Loss: 0.0568\n",
      "Model predicted ranks:  tensor([4, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 7 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 72/100 - Loss: 0.0530\n",
      "Model predicted ranks:  tensor([2, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 6 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 73/100 - Loss: 0.0517\n",
      "Model predicted ranks:  tensor([4, 2, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 7 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 74/100 - Loss: 0.0478\n",
      "Model predicted ranks:  tensor([2, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 7 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 75/100 - Loss: 0.0484\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 76/100 - Loss: 0.0483\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 77/100 - Loss: 0.0463\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 78/100 - Loss: 0.0444\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 79/100 - Loss: 0.0426\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 80/100 - Loss: 0.0418\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 81/100 - Loss: 0.0375\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 82/100 - Loss: 0.0346\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 83/100 - Loss: 0.0357\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 2, 0, 2]) Number of correct 8 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 84/100 - Loss: 0.0359\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 85/100 - Loss: 0.0296\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 86/100 - Loss: 0.0305\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 87/100 - Loss: 0.0321\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 88/100 - Loss: 0.0280\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 89/100 - Loss: 0.0264\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 90/100 - Loss: 0.0253\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 91/100 - Loss: 0.0239\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 92/100 - Loss: 0.0232\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 93/100 - Loss: 0.0231\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 94/100 - Loss: 0.0199\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 2]) Number of correct 9 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 95/100 - Loss: 0.0218\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 96/100 - Loss: 0.0217\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 97/100 - Loss: 0.0174\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 98/100 - Loss: 0.0177\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 99/100 - Loss: 0.0194\n",
      "Model predicted ranks:  tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3]) Number of correct 10 target: tensor([4, 4, 0, 2, 1, 1, 2, 3, 0, 3])\n",
      "Epoch 100/100 - Loss: 0.0156\n",
      "Final Prediction:\n",
      " tensor([[[0.9500, 0.9600, 0.8400, 0.8300, 0.0200]],\n",
      "\n",
      "        [[0.9700, 0.9700, 0.8500, 0.8400, 0.0200]],\n",
      "\n",
      "        [[0.1400, 0.0600, 0.0900, 0.0700, 0.0200]],\n",
      "\n",
      "        [[0.9200, 0.8200, 0.2800, 0.1800, 0.0300]],\n",
      "\n",
      "        [[0.9100, 0.1300, 0.1200, 0.1300, 0.0200]],\n",
      "\n",
      "        [[0.8600, 0.1800, 0.1300, 0.1000, 0.0300]],\n",
      "\n",
      "        [[0.9200, 0.8900, 0.3300, 0.1400, 0.0200]],\n",
      "\n",
      "        [[0.9300, 0.9300, 0.7400, 0.1300, 0.0300]],\n",
      "\n",
      "        [[0.0900, 0.0600, 0.0700, 0.0700, 0.0100]],\n",
      "\n",
      "        [[0.9300, 0.9500, 0.7400, 0.1000, 0.0300]]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q1/8y15w8vx0jz649jwwx748zyr0000gn/T/ipykernel_92553/2087499728.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q = torch.tensor(X_grouped[:, 0, :, :]).clone().detach()\n",
      "/var/folders/q1/8y15w8vx0jz649jwwx748zyr0000gn/T/ipykernel_92553/2087499728.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k = torch.tensor(X_grouped[:, 1, :, :]).clone().detach()\n",
      "/var/folders/q1/8y15w8vx0jz649jwwx748zyr0000gn/T/ipykernel_92553/2087499728.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v = torch.tensor(X_grouped[:, 2, :, :]).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "def create_ordinal_target(num, length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a batch of binary target vectors where each vector contains 1s from index 0 to num[i]-1\n",
    "    and 0s for the rest, given a list of integers and a fixed vector length.\n",
    "\n",
    "    Args:\n",
    "        num (list): A list of integers specifying how many 1s each vector should contain.\n",
    "        length (int): The total length of each output vector (or the number of ranking placements)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (len(num), length) with 1s up to each corresponding\n",
    "                      value in `num`, and 0s elsewhere.\n",
    "\n",
    "    Example:\n",
    "        >>> create_target([2, 4], 5)\n",
    "        tensor([[1, 1, 0, 0, 0],\n",
    "                [1, 1, 1, 1, 0]])\n",
    "    \"\"\"\n",
    "    assert not isinstance(num, int), f\"Num must be either tensor / numpy list.\"\n",
    "    num_data = len(num)\n",
    "    vec = torch.zeros((num_data, length), dtype=torch.float, requires_grad=False)\n",
    "    for idx, d in enumerate(num):\n",
    "        vec[idx, :d] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "class Ranker(nn.Module):\n",
    "    def __init__(self, num_ranks, emb_dim, num_heads, dropout_rate=0.6, seq_len=25):\n",
    "        super(Ranker, self).__init__()\n",
    "        self.num_ranks = num_ranks\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # batch_first means that input and output follows (1, seq_length, emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            self.emb_dim,\n",
    "            self.num_heads,\n",
    "            dropout=self.dropout_rate,\n",
    "            batch_first=True,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.fn = nn.Linear(self.emb_dim, self.num_ranks, bias=True)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert Q.size(1) == self.seq_len and Q.size(2) == self.emb_dim, (\n",
    "            f\"Expected Q shape [batch_size, {self.seq_len}, {self.emb_dim}](batch_size, seq_len, emb_dim) but received: {Q.shape}\"\n",
    "        )\n",
    "        assert Q.shape == K.shape == V.shape\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "        attn_output, attn_weights = self.attn(Q, K, V)\n",
    "        assert (\n",
    "            attn_output.size(0) == batch_size\n",
    "            and attn_output.size(1) == self.seq_len\n",
    "            and attn_output.size(2) == self.emb_dim\n",
    "        ), (\n",
    "            f\"Expected Attn output shape to be (batch_size {self.seq_len}, {self.emb_dim}) but received {attn_output.shape}\"\n",
    "        )\n",
    "        assert (\n",
    "            attn_weights.size(0) == batch_size\n",
    "            and attn_weights.size(1) == attn_weights.size(2) == self.seq_len\n",
    "        ), (\n",
    "            f\"Expected Attn output weight shape to be (batch_size {self.seq_len}, {self.seq_len}) but received {attn_weights.shape}\"\n",
    "        )\n",
    "\n",
    "        output = attn_output.softmax(-1)\n",
    "        x = self.fn(output)\n",
    "        assert (\n",
    "            x.dim() == 3 and x.size(1) == self.seq_len and x.size(2) == self.num_ranks\n",
    "        )\n",
    "        x = x.sum(axis=1).unsqueeze(1)\n",
    "        assert x.dim() == 3 and x.size(0) == batch_size and x.size(2) == self.num_ranks\n",
    "\n",
    "        # print(f\"Prediction:\\n\", torch.round(x * 1e2) / 1e2)\n",
    "        return x.sigmoid()\n",
    "\n",
    "\n",
    "def display_results(model_output, target, threshold: float = 0.65):\n",
    "    \"\"\"counts by batch input higher than threshold\"\"\"\n",
    "    results = (model_output > threshold).sum(axis=-1)\n",
    "    target_results = (target > threshold).sum(axis=-1)\n",
    "    print(\n",
    "        \"Model predicted ranks: \",\n",
    "        results.flatten(),\n",
    "        \"Number of correct\",\n",
    "        (results.flatten() == target_results.flatten()).sum(axis=0).item(),\n",
    "        \"target:\",\n",
    "        target_results.flatten(),\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_pattern(levels, emb_dim, noise_scale=0.01):\n",
    "    pattern = torch.cat(\n",
    "        [torch.ones(length, emb_dim) * val for length, val in levels], dim=0\n",
    "    )\n",
    "    return pattern + noise_scale * torch.randn_like(pattern)\n",
    "\n",
    "\n",
    "def mock_dataset(batch_size, emb_dim, seq_len):\n",
    "    assert batch_size == 3, \"This mock currently supports batch_size=3 for simplicity\"\n",
    "\n",
    "    # Define sequence lengths\n",
    "    spike_pattern = torch.ones(seq_len, emb_dim) * 0.1\n",
    "    spike_pattern[seq_len // 2] = 1.5  # Spike at center timestep\n",
    "\n",
    "    base_patterns = [\n",
    "        generate_pattern([(seq_len, 0.1)], emb_dim),\n",
    "        spike_pattern + 0.01 * torch.randn_like(spike_pattern),\n",
    "        generate_pattern([(seq_len, 0.9)], emb_dim),\n",
    "    ]\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    q = torch.stack(\n",
    "        [pattern + 0.01 * torch.randn_like(pattern) for pattern in base_patterns]\n",
    "    )\n",
    "    k = torch.stack(\n",
    "        [pattern + 0.01 * torch.randn_like(pattern) for pattern in base_patterns]\n",
    "    )\n",
    "    v = torch.stack(\n",
    "        [pattern + 0.01 * torch.randn_like(pattern) for pattern in base_patterns]\n",
    "    )\n",
    "\n",
    "    target = torch.tensor(\n",
    "        [[[1, 0, 0, 0, 0]], [[1, 1, 0, 0, 0]], [[1, 1, 1, 0, 0]]], dtype=torch.float\n",
    "    )\n",
    "\n",
    "    print(\"Q:\\n\", q.shape)\n",
    "    print(\"K:\\n\", k.shape)\n",
    "    print(\"V:\\n\", v.shape)\n",
    "    print(\"Target:\\n\", target.shape)\n",
    "    print(\"--\" * 10)\n",
    "    return q, k, v, target\n",
    "\n",
    "\n",
    "def formally_mock_data(batch_size, num_ranks, emb_dim, seq_len):\n",
    "    # mock dataset generation args\n",
    "    X, y = make_classification(\n",
    "        n_samples=batch_size,\n",
    "        n_features=emb_dim * seq_len * 3,\n",
    "        n_informative=num_ranks,\n",
    "        n_redundant=1,\n",
    "        n_classes=num_ranks,\n",
    "        class_sep=1.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    X_grouped = torch.tensor(X, dtype=torch.float).view(batch_size, 3, seq_len, emb_dim)\n",
    "    print(X_grouped.shape)\n",
    "    q = torch.tensor(X_grouped[:, 0, :, :]).clone().detach()\n",
    "    k = torch.tensor(X_grouped[:, 1, :, :]).clone().detach()\n",
    "    v = torch.tensor(X_grouped[:, 2, :, :]).clone().detach()\n",
    "    print(q.shape)\n",
    "    target = create_ordinal_target(y, length=num_ranks)\n",
    "    target = target.unsqueeze(1)\n",
    "    return q, k, v, target\n",
    "\n",
    "\n",
    "def trial(max_epoch: int = 100):\n",
    "    emb_dim = 50\n",
    "    num_heads = 2\n",
    "    dropout_rate = 0.6\n",
    "    num_ranks = 5\n",
    "    seq_len = 25\n",
    "    batch_size = 10\n",
    "\n",
    "    q, k, v, target = formally_mock_data(\n",
    "        batch_size=batch_size, num_ranks=num_ranks, emb_dim=emb_dim, seq_len=seq_len\n",
    "    )\n",
    "    model = Ranker(num_ranks, emb_dim, num_heads, dropout_rate, seq_len)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(int(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Assuming model(q, k, v) returns predictions of shape (batch_size, seq_len, 1)\n",
    "        output = model(q, k, v)\n",
    "        loss = criterion(output, target)\n",
    "        display_results(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{max_epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Final Prediction:\\n\", torch.round(output * 1e2) / 1e2)\n",
    "\n",
    "\n",
    "# qkv shape [batch_size, seq_len, emb_dim]\n",
    "trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      " torch.Size([3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 4\n",
    "num_heads = 2\n",
    "dropout_rate = 0.6\n",
    "num_ranks = 3\n",
    "seq_len = 25\n",
    "batch_size = 3\n",
    "\n",
    "assert emb_dim % num_heads == 0, (\n",
    "    f\"Number of emb_dim must be divisible by num heads, received emb_dim={emb_dim} and num_heads={num_heads}\"\n",
    ")\n",
    "\n",
    "attn = nn.MultiheadAttention(\n",
    "    emb_dim, num_heads, dropout=dropout_rate, batch_first=True, bias=False\n",
    ")\n",
    "fn = nn.Linear(emb_dim, num_ranks)\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, emb_dim)\n",
    "k = torch.randn(batch_size, seq_len, emb_dim)\n",
    "v = torch.randn(batch_size, seq_len, emb_dim)\n",
    "target = torch.tensor([[[1, 0, 0]], [[1, 1, 0]], [[1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "# print(\"Q:\\n\", q)\n",
    "# print(\"K:\\n\", k)\n",
    "# print(\"V:\\n\", v)\n",
    "print(\"Target:\\n\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f8a3f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([3, 25, 4])\n",
      "Attention output weights shape: torch.Size([3, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "output = attn(q, k, v)\n",
    "# outputs tuple with length = 2\n",
    "# idx 0 = attn output after softmax = shape = (1, 25, 4) = (batch_size, seq_len, emb_dim)\n",
    "# idx 1 = attn weights = shape = (1, 25, 25) = (batch_size, seq_len, seq_len)\n",
    "\n",
    "print(\"Attention output shape:\", output[0].shape)\n",
    "print(\"Attention output weights shape:\", output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3723d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(output[0]).sum(axis=1).softmax(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a8515e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = fn(output[0]).sum(axis=1).softmax(-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ad72c8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2100, 0.0000, 0.7900],\n",
       "        [0.9900, 0.0000, 0.0100],\n",
       "        [0.9800, 0.0000, 0.0200]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(x * 1e2) / 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3487c",
   "metadata": {},
   "source": [
    "# Comparing Models Example Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b34919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleField:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "        )\n",
    "        self.infer_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"HuggingFaceTB/SmolLM-135M-Instruct\", attn_implementation=\"eager\"\n",
    "        )\n",
    "        self.assist_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"HuggingFaceTB/SmolLM-135M-Instruct\", attn_implementation=\"eager\"\n",
    "        )\n",
    "\n",
    "        assert id(self.assist_model) != id(self.infer_model)\n",
    "        assert (\n",
    "            self.assist_model.model.layers[0].self_attn.q_proj.weight.data_ptr()\n",
    "            != self.infer_model.model.layers[0].self_attn.q_proj.weight.data_ptr()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1b476e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BattleField at 0x32f7a95b0>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BattleField()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
